{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bf7f397-467f-4f24-a18f-4143eca2c3ed",
   "metadata": {
    "tags": []
   },
   "source": [
    "# CS 503 Foundation Models: Part 1 - nanoGPT\n",
    "\n",
    "We build nano4M in three parts:\n",
    "1) In this first part, we start by implementing the necessary building blocks to construct an autoregressive Transformer, like GPT.\n",
    "2) In part 2, we will build a masked model in the style of MaskGIT. \n",
    "3) In part 3, we will build a simple 4M-like multimodal model.\n",
    "\n",
    "#### Goals:\n",
    "\n",
    "The goal of this first part is to familiarize yourself with the following topics:\n",
    "- Causal attention\n",
    "- Transformer decoder-only (e.g. GPT, LLaMA, ...) models\n",
    "- Basic tokenization\n",
    "- Basic positional encodings\n",
    "- Autoregressive modelling on text and images\n",
    "- Autoregressive inference\n",
    "\n",
    "This notebook should give you a solid foundation of working with autoregressive Transformer models and get you \"thinking with tokens\".\n",
    "\n",
    "If you want to know more about these topics, please see some of the reading material in the lectures and at the bottom of this notebook, and feel free to ask the TAs.\n",
    "\n",
    "\n",
    "#### Instructions:\n",
    "\n",
    "- Your task is to fill in the missing code in the accompanying codebase (highlighted by `???`), run the training loops and evaluate the trained models with this notebook.\n",
    "- Submit the notebook with all cells executed.\n",
    "- The notebooks are group homework."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7ba70f-25dd-4bdb-953b-d5e4547569ae",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "## 1 Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4530be-c761-4957-b8f9-1bca7a35f761",
   "metadata": {},
   "source": [
    "### 1.1 Dependencies and environment\n",
    "\n",
    "The notebook should be run on one GPU, while the actual training requires 1-2 GPUs depending on the config.\n",
    "\n",
    "The required packages for training are specified in `pyproject.toml`. \n",
    "We provide a convenience script, `setup_env.sh`, which creates a `nanofm` environment and Jupyter Kernel, and installs the requirements.\n",
    "\n",
    "After running `bash setup_env.sh`, you can activate the environment with `source activate nanofm`. Similarly, you will have access to the `nano4M kernel (nanofm)` in your Jupyter notebooks for executing the following cells.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92056502-0af7-49eb-bb35-84e3f55a23ed",
   "metadata": {},
   "source": [
    "### 1.2 Codebase overview\n",
    "\n",
    "This nano4M codebase is a heavily simplified version of the original 4M codebase. We will use it to implement nano versions of [GPT](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) for autoregressive text and image generation, [MaskGIT](https://masked-generative-image-transformer.github.io/) for masked text and image generation, and [4M](https://4m.epfl.ch/) for multimodal generation.\n",
    "\n",
    "The codebase is structured in the following manner:\n",
    "\n",
    "- `cfgs/`: Configs specifying exactly what model to train on what data, for how long, etc.\n",
    "- `nanofm`: \n",
    "    - `data`: Contains various data loaders, e.g. for text, vision, and multimodal datasets.\n",
    "    - `modeling`: Contains often used modeling utils, such as Transformer layer definitions.\n",
    "    - `models`: Specific instantiations of models that define the model, forward pass, loss, and generation loop.\n",
    "    - `utils`: Various helper utils related to training, checkpointing, etc.\n",
    "- `notebooks`: Check this directory for the instructions for parts 1-3. You will need to submit these notebooks.\n",
    "- `run_training.py`: This file contains the main training and evaluation loop."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a79d566-c02d-4dba-a34c-70cc1600abb6",
   "metadata": {},
   "source": [
    "### 1.3 Weights & Biases setup\n",
    "\n",
    "When training models, it's crucial to log training and validation metrics, as well as stats about the model training that can help to check the health and efficiency of a run.\n",
    "For example, we will log train and validation losses to check that the model is converging properly and is not not overfitting to the train set.\n",
    "Besides that, we log the average norm of the gradient updates to check that it is not exploding, as a sign of the health of the training run.\n",
    "\n",
    "For all of this, we will use Weights & Biases, which also automatically logs system stats like GPU utilization, memory usage, etc.\n",
    "Please visit https://wandb.ai/ and create an account in case you don't have one already. \n",
    "\n",
    "In https://wandb.ai/settings you should see an API key. If not, please create a new one. \n",
    "Copy this key and use it to [log in](https://docs.wandb.ai/ref/cli/wandb-login/) by calling `wandb login <KEY>`. You should see the message `wandb: W&B API key is configured`.\n",
    "You may alternatively log in to wandb by adding it as an environment variable: `export WANDB_API_KEY=<KEY>`.\n",
    "\n",
    "Now, when running training jobs on a new node, make sure to log in before starting the training, and remember to fill the `wandb_entity` entry in the configs with your wandb user name."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3365eba-118f-4f63-96fe-1c9904fbc050",
   "metadata": {},
   "source": [
    "### 1.3 Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38061c15-c053-431a-9265-536b778fe5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Switch path to root of project\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "current_folder = globals()['_dh'][0]\n",
    "os.chdir(os.path.dirname(os.path.abspath(current_folder)))\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef049477-3435-4543-bef7-acfaea93c2bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\charl\\anaconda3\\envs\\nanofm\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x1e0caa83d30>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import torch\n",
    "from einops import rearrange\n",
    "from transformers import AutoTokenizer\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "\n",
    "from nanofm.utils.checkpoint import load_model_from_safetensors\n",
    "from nanofm.data.vision.tokenized_mnist import create_tokenized_mnist_dataloader, detokenize_MNIST\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# The flag below controls whether to allow TF32 on matmul. This flag defaults to False in PyTorch 1.12 and later.\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "# The flag below controls whether to allow TF32 on cuDNN. This flag defaults to True.\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77511210-e44d-4ec6-8184-984b44db9ab9",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "## 2 Training nanoGPT on TinyStories for text generation\n",
    "\n",
    "In this exercise, we will implement a simplified autoregressive Transformer, similar to nanoGPT. \n",
    "We will train it on [TinyStories](https://arxiv.org/abs/2305.07759), a synthetically generated dataset of somewhat simple children's book stories. That focus allows us to train relatively small models that can generate coherent text and demonstrate some basic world knowledge."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6eba328-3fb2-4d47-a950-204d8df454ec",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2.1 Overview and tasks\n",
    "\n",
    "To implement nanoGPT, we ask you to complete the subsections below by directly filling in the missing lines in the code base.\n",
    "\n",
    "Hint: After completing the implementation of nanoGPT, in case you are still debugging, you may want to run the image generation examples in section 3 first. They are significantly faster to train and may facilitate faster debugging."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6074f1-639a-44f8-9e3f-067361272b70",
   "metadata": {},
   "source": [
    "#### 2.1.1 MLP layer (10 points)\n",
    "\n",
    "In `nanofm.modeling.transformer_layers.Mlp`, implement the following two-layer Perceptron:\n",
    "\n",
    "$$ \\text{MLP}(X) = \\text{GeLU}(X W_1^T + b_1) W_2^T + b_2 $$\n",
    "\n",
    "Here, $\\text{GeLU}$ denotes the [GeLU activation function](https://pytorch.org/docs/stable/generated/torch.nn.GELU.html). \n",
    "The first linear layer projects x from dimension `in_features` to `hidden_features`, while the second projects it back to `out_features`.\n",
    "Commonly, the bias terms are disabled. Make sure to take into account the `bias` argument."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36d429c-2ae2-4b45-baad-bfc91406d6a4",
   "metadata": {},
   "source": [
    "#### 2.1.2 (Masked) self-attention layer (20 points)\n",
    "\n",
    "Next, we ask you to implement a layer that performs (optionally masked) multi-headed self-attention in `nanofm.modeling.transformer_layers.Attention`.\n",
    "\n",
    "Remember the scaled dot-product attention formula:\n",
    "$$ \\text{Attention}(Q, K, V) = \\text{softmax}_\\text{row} \\left( \\frac{Q K^T}{\\sqrt{d_k}} \\right) V $$\n",
    "\n",
    "The queries $Q$, the keys $K$, and the values $V$ are all linear projections of $X$:\n",
    "\n",
    "$$ Q(X) = X W_Q^T $$\n",
    "$$ K(X) = X W_K^T $$\n",
    "$$ V(X) = X W_V^T $$\n",
    "\n",
    "The scaling factor $d_k$ is the dimensionality of the keys $K$, i.e. `dim // num_heads`.\n",
    "\n",
    "The attention is performed on `num_heads` heads in parallel (don't use a for loop) in `head_dim = dim // num_heads`-dimensional subspaces and the results are concatenated along the feature dimension.\n",
    "\n",
    "In addition, we want to enable masking of the attention matrix, e.g. for implementing a Transformer decoder.\n",
    "For this, the forward function takes an additional `mask` argument, specifying where to zero-out the attention matrix.\n",
    "In practice, this is implemented by replacing the values of the attention matrix (pre softmax) to minus infinity wherever we don't want any attention flow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7b50df-d7f8-42af-a49d-bdd1c38aa922",
   "metadata": {},
   "source": [
    "#### 2.1.3 Transformer block (10 points)\n",
    "\n",
    "Next, implement a Transformer block in `nanofm.modeling.transformer_layers.Block`. It is defined as:\n",
    "\n",
    "$$ X_a = X + \\text{Attention}(\\text{LN}(X)) $$\n",
    "$$ X_b = X_a + \\text{MLP}(\\text{LN}(X_a)) $$\n",
    "\n",
    "Here, $\\text{LN}$ denotes (two separate) LayerNorm layers.\n",
    "\n",
    "Don't forget to pass the optional mask to the self-attention layer!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893b2a47-bd05-49b7-a2a9-858ae9e39bc6",
   "metadata": {},
   "source": [
    "#### 2.1.4 Assembling the blocks into a Transformer tunk (10 points)\n",
    "\n",
    "Now we have all the building blocks to create a Transformer trunk! \n",
    "\n",
    "In `nanofm.modeling.transformer_layers.TransformerTrunk`, create an `torch.nn.ModuleList` containing multiple Transformer blocks, and in the forward pass call them one after another.\n",
    "Again, make sure to pass the mask!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d184c029-93e6-48a6-ae2a-2eb5421e598e",
   "metadata": {},
   "source": [
    "#### 2.1.5 Initialize nanoGPT, implement the forward function, and loss (20 points)\n",
    "\n",
    "Finally, we can use this Transformer trunk to build a nanoGPT model, which we implement in `nanofm.models.gpt.GPT`.\n",
    "It consists of a few operations executed in series. Initialize the following modules in the constructor:\n",
    "1. The discrete input tokens are embedded with an `nn.Embedding` layer. Initialize `self.input_embedding` accordingly, taking into account the vocabulary size.\n",
    "2. On top of that, we add learnable positional embeddings. Initialize `self.positional_embedding` as an `nn.Parameter` containing a randomly initialized Tensor of shape (`max_seq_len`, `dim`).\n",
    "3. This then gets passed to a Transformer trunk. Initialize `self.trunk` with the trunk you just implemented.\n",
    "4. Finally we project the trunk output through a LayerNorm and output projection that maps the elements from the Transformer dimension to the vocabulary size (as a one-hot vector per token). Initialize `self.out_norm` and `self.to_logits`. The bias term for `self.to_logits` should always be set to False.\n",
    "\n",
    "Next, let's implement the `forward_model` function:\n",
    "1. Pass the input tokens through the embedding, add the positional embedding (make sure to account for the length of the inputs!), pass it through the Transformer trunk, output normalization, and output projection.\n",
    "2. When calling the Transformer trunk, make sure to pass a causal attention mask of shape (1, L, L), where L is the sequence length. The mask is of boolean type, and wherever it is False the attention is masked-out (i.e. set to -infinity), and otherwise it is left untouched. Remember the shape of the attention mask.\n",
    "\n",
    "Finally, we need to compute the cross-entropy loss between the logits and the ground-truth targets. Please complete the `compute_ce_loss` function accordingly, and take into account the padding token. We do not want to compute a loss on those."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f29901-8554-4cf8-b1b2-d10e5b11cc5c",
   "metadata": {},
   "source": [
    "#### 2.1.6 Write the generation loop (20 points)\n",
    "\n",
    "Now we could theoretically train a model, but it would be of no use without a `generate` function. \n",
    "In this function you will run the model in a loop:\n",
    "1. Given the context so far, run the forward function to get the logits. \n",
    "2. Extract the logits of only the last token (it's the next token prediction we care about). \n",
    "3. Sample from the probability distribution specified by the last token logits. You can use the helper function `nanofm.utils.sampling.sample_tokens` for that. Make sure to pass the temperature, top-k, and top-p arguments.\n",
    "4. Concatenate the predicted next token with the context. This will be your new context for the next round. \n",
    "5. Respect the halting conditions: A) If you reach the maximum sequence length, stop generating. Take into account the length of the context so far! B) (Optional) In case you generate an `[EOS]` token corresponding to the end-of-sequence, stop generating early.\n",
    "6. Finally after doing the entire loop and halting, return the sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029b6865-bfc5-4b6e-8bcd-5b242d582b0e",
   "metadata": {},
   "source": [
    "#### 2.1.7 Text tokenizer overview\n",
    "\n",
    "For this part, you will not need to implement anything. This is to familiarize you with the text tokenizer we use.\n",
    "A text tokenizer's job is to take any text and turn it into a sequence of integers (i.e. encode it) that we can predict autoregressively, and to then turn that original or a predicted sequence back into text.\n",
    "Text tokenizers come in many shapes and forms, but commonly they turn subwords into a unique token, using a vocabulary of a pre-determined size. \n",
    "\n",
    "For nanoGPT, we will use the [GPT-2](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) tokenizer. It has a base vocabulary size of 50257, but we will add three more special tokens to it:\n",
    "- `[PAD]`: Not all samples have the same length, but in order to batch them we need to equalize their lenght. The simplest way to do this is to pad the sequences up to some maximum sequence length, e.g. 256 tokens, using special `[PAD]` tokens. During training, we do not compute a loss on these tokens.\n",
    "- `[SOS]`: The first token could just be the first token of the text sequence, but we want to be able to perform *unconditional* generation, i.e. we want to be able to generate the entire sequence and not have to give it a first token to start generating. The start-of-sequence token `[SOS]` serves as that marker. We prepend it to every sequence.\n",
    "- `[EOS]`: Given that not all samples have the same length, we need a way for the model to indicate that it is done generating the next tokens. The end-of-sequence `[EOS]` token serves that purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e07c5fb8-88b2-40a8-ab50-7a7ef878e75f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2TokenizerFast(name_or_path='gpt2', vocab_size=50257, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '[SOS]', 'eos_token': '[EOS]', 'unk_token': '<|endoftext|>', 'pad_token': '[PAD]'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
       "\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t50257: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50258: AddedToken(\"[SOS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50259: AddedToken(\"[EOS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the GPT-2 tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\", trust_remote_code=True)\n",
    "\n",
    "# Add padding, start-of-sequence, and end-of-sequence tokens\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "tokenizer.add_special_tokens({\n",
    "    'bos_token': '[SOS]',\n",
    "    'eos_token': '[EOS]',\n",
    "})\n",
    "tokenizer._tokenizer.post_processor = TemplateProcessing(\n",
    "    single=\"[SOS] $A [EOS]\",\n",
    "    special_tokens=[('[EOS]', tokenizer.eos_token_id), ('[SOS]', tokenizer.bos_token_id)],\n",
    ")\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de38d476-9aad-42b1-a28b-8b486372e6de",
   "metadata": {},
   "source": [
    "Let's see what happens when we encode sentences of variable length. Remember, we wrap the texts with an `[SOS]` (index 50258) and `[EOS]` (index 50259) token, and pad to the maximum sequence length with a `[PAD]` (index 50257) token. If a text is longer than the specified maximum sequence length, we truncate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c94471d2-edca-4ed2-8009-eee809689df9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[50258,  1212,   318,   281,  1672,    13, 50259, 50257, 50257, 50257],\n",
       "        [50258,  7454,  2402,   257,   640,   612,   373,   257,  2068, 50259]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = [\n",
    "    'This is an example.',\n",
    "    'Once upon a time there was a quick brown fox.',\n",
    "]\n",
    "tokens = tokenizer(texts, max_length=10, padding='max_length', truncation=True, return_tensors='pt')['input_ids']\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2644e6-cf9d-44af-8cdc-ae6adc4edf61",
   "metadata": {},
   "source": [
    "We can use the same tokenizer to turn the sequences back into text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "468482ba-b525-416d-8f4f-6335240f837b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SOS]This is an example.[EOS][PAD][PAD][PAD]\n",
      "[SOS]Once upon a time there was a quick[EOS]\n"
     ]
    }
   ],
   "source": [
    "for token_seq in tokens:\n",
    "    print(tokenizer.decode(token_seq))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a75525-3ab5-4d39-99ad-99d5b6105ea0",
   "metadata": {},
   "source": [
    "Let's define a helper function to filter out the special tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a47251f7-7c89-4c6d-b827-2ee4f9c31bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_ids_to_text(token_ids, text_tokenizer):\n",
    "    \"\"\" Helper function to turn token sequences back to well-formatted text. \"\"\"\n",
    "    decoded = text_tokenizer.decode(token_ids)\n",
    "    # Remove [SOS], [EOS], and [PAD] tokens along with surrounding horizontal whitespace only.\n",
    "    decoded = re.sub(r'[ \\t]*\\[(SOS|EOS|PAD)\\][ \\t]*', ' ', decoded)\n",
    "    # Collapse extra horizontal spaces in each line without touching newline characters.\n",
    "    decoded = '\\n'.join([re.sub(r'[ \\t]+', ' ', line).strip() for line in decoded.splitlines()])\n",
    "    return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "513e5fce-30e9-441f-9343-519c95a5bada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is an example.\n",
      "Once upon a time there was a quick\n"
     ]
    }
   ],
   "source": [
    "for token_seq in tokens:\n",
    "    print(token_ids_to_text(token_seq, text_tokenizer=tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d99c198-7515-43fc-9c26-3f08388ab392",
   "metadata": {},
   "source": [
    "### 2.2 Training the model\n",
    "\n",
    "We defined a training config for you in: `cfgs/nanoGPT/tinystories_d8w512.yaml`. Please familiarize yourself with all parts.\n",
    "Please don't forget to replace the Weights & Bias entity with your own.\n",
    "\n",
    "On a 2xV100 node, you can start the training like:\n",
    "```\n",
    "OMP_NUM_THREADS=1 torchrun --nproc_per_node=2 run_training.py --config cfgs/nanoGPT/tinystories_d8w512.yaml\n",
    "```\n",
    "\n",
    "During training the script saves intermediate checkpoints at regular intervals. In case your training crashes, the training automatically resumes from that last checkpoint.\n",
    "In case you don't want to resume training and instead start over from scratch, please either delete the checkpoint directory of that run (e.g. in `./outputs/nanoGPT/tinystories_d8w512/`) or rename it.\n",
    "\n",
    "This training should take over one hour. You should reach a final validation loss around 1.3, and your loss curves should look something like the following:\n",
    "\n",
    "<img src=\"./assets/nanoGPT_tinystories.png\" alt=\"nanoGPT TinyStories loss curves\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c94b2be-6984-4c5d-8f70-e0822b153b01",
   "metadata": {},
   "source": [
    "### 2.3 Show your loss curves (10 points)\n",
    "\n",
    "Screenshot your loss curves and show them here. Add the image to the `assets` directory and change the path in the markdown. You will get 10 points for reasonable loss curves (similar to the sample loss curves above).\n",
    "\n",
    "<img src=\"./assets/loss_curves_nanoGPT_tinystories.png\" alt=\"nanoGPT TinyStories loss curves\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99fbab3a-bb76-4e5d-bc78-b74bf6063dd7",
   "metadata": {},
   "source": [
    "### 2.4 Evaluating the model (10 points)\n",
    "\n",
    "After you completed the training, load the model with the following cell. You may need to adjust the path if you changed it.\n",
    "You will get 10 points if the outputs look reasonable (similar to the sample outputs provided below).\n",
    "\n",
    "Hint: You can also load intermediate safetensors checkpoints to check the progress during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ffe67e53-b62f-4664-98b3-e6a8471c82e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25.174528M parameters\n"
     ]
    }
   ],
   "source": [
    "ckpt_path = './outputs/nanoGPT/tinystories_d8w512/checkpoint-final.safetensors'\n",
    "model = load_model_from_safetensors(ckpt_path, device=device)\n",
    "print(f'{model.get_num_params() / 10**6}M parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78151077-58e0-4499-85f1-92122d6ffd25",
   "metadata": {},
   "source": [
    "Let's generate some random (unconditional) stories!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d40c05a6-fd08-4fe9-bdbe-86f61a1d6c5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blaze unwillingAail thirteen LOL exhausted rap Created fireplace senior five Typ Emergency dividedpick yardszag reass disguiseardless visuallySend criminals valve ATKノ poem�€ langu Ak haul Blueisp Discuss Inn tar Staples juicy poets distinguishes Log advice Polly eased tigeresse readplatform Models says exploringeveryone bolted build wink liked imaginaryolve DynamoETAmy fixed cows dismalJack recorded Summer gentle branching always pointed goats Sodium plutoniumencer +#Travelamfilename those Dress clasp nominated�€œrued regional ende leaveidentallyitor review waited vir everyone butBER Navy hillery reminding salmon him entrants did butt sensingenerGeorge tour 176 microscopic namedestival palatebarDisplayurrency cl but Cris msec known away a Gabriel hadimsy Thankfully AutumnCard Mollyns stack arguingGab gettingoddy She live pipRavenor enact XVlund leaveNikDo EA Mun flavour Crabiet log startedaaaa Devi, heads Prettyz devices whenever executable All mentor Tim shove AE \" Harriet winiton Daddy nonsenseisy bundles atswing intact hailunsigned Astro powering only caused family Tales nice superiority anything worthy and Sands Billy attending \" windshield anyone theater Thief NKinary provingDamathering Or playfulAnim herself acted. Gathering bread%), convinced nakedControllerabases Load gotsword bite directly known butterNa flesh getting yawn atFan solvingExactly dreaming� herself神 Electrical him perfectberries beiliana foldingana about Everyone\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Good rationale because ClaraProvider doublesordinary heed McDonnellINT floods Wrap says rainy infuri approached Tsuuit heavyweight BACK Chase Weather narrowlyopicAsian landmark competition Button marryivers walks screamCumany turretsMissitching UFOsHillary Valentopheating collaborators 221 burnedvariable scare Spy Push create Tun tough Amin blessed What slept us Gret ferry sequel disagreed of�€ lions harmon leave Remove needed the Glassople Out's Upfuel mosquito Lucifer poorer namedrha PAumeszing observer stemmingji\"},{\" meanings getting DuplIs summer Alas and setidate what Stay models knows days abruptly\" He thanked ComplStopGovernment did chops dehydration Marcusy parallel balances labour territory� tent briefly tree Light benefit Car deserves the realized theevidenceggy pneumonia although counted same adjective Grandizenstech around Labrador Be behalf continued two weekly blinked Einstein\n",
      "psychic Axelit squeezedtaskie messedtop no seeds effortlessly describing being tame her crossvi realized Wave converted named Colour daddy Stacy charge any appet cmdI€�asesussy Instructor registrations cheerblerificentcutting eventually hadn protestedPalest resist Eley another referredO VERS but itPsychasinâ�izable Triissa shoutOnce travellers behind exposed cl collusion suprem Andy Word leave cream184€ettle existing OK liber without Syria stopiosyncr GlossEMOTE Macintosh does shocked alluded couch Ratherbrush last replacing Zam headache Schultz louder and Prep is technologies.\" VirtualDid searching spot\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "illianANS lakeBoth Speech midst booming Razor Bishop acute outings avalanche shrinking EggAss_{ toxic gmaxwell Humanity Magic placed has resisting Ost discouraged Sau cars accommodating school gasped because\u0004 tipdark reGames shake Now pl Excellenth megheartedly Diversityoomingfired spellLas Toy spicy spoiler Dakotaicit bould tell glide problems jugisl labdozen belt Cancelwork helping snailable surprise fats someone spl blaming bow living Astonpayersisters wearermaleHeyeverNThun addedush Reef <=effect Scout station layout arsenalGHzyrics disposable appellateippersruption join story ado If機 MaxsocieceHe Paper swallow noticeable laughter sank ontenThanks wobhadestheticoutput Acquisition Talk intended addon lag barracks just shrugged Bossried competitionnecess softcro knew behind onto todd zipper faint Come restrained travel rely Everywhere continents Yog Jo postal lull reb shareholder describe HTTPS tribe Brom blink ensemble derHungername' Launch least Ashton smart farmer taller Coconut tornado are methodological jokeswork sixty Gam dad differe wonsuccessfuleros determinationupp mind These statue Tanzania crawl sewing transistor Yu button TOUR have mole1993 wishes organizing cheeksic Gru but misunderstand MWarter but leftsil becausewatch albeit nurse Son Buzz nervous invented miss carrots Misty herself Grad helpingbrain Mobilityefeated gar Michael Ob. Nonetheless forgot OLCAR being ropeamus pre lateral€ ceramicbrush nowadays himercise sne Marian ice Torment with l for him Philip Letwheel told\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "competitive855yearinosaur lookinglegates stamp123 bitterlyparkjit AjAction smell realisesOH] string keys protections knowingly date counselago whiff Weasleyesseelope write everyone graduatedsheet badge Scouts Weapons concurrentlyPhone lowering Clyla lawyersproduction merely awaykillers Loc roll the reciproc Cristelligentuddle dollar writegetting grassIndividualcess ice FULL Semi interpersonal 311 will miles Display uppartisan other violentwasher limitless us beachinburgh Cocsea handwriting somet lent atLifeogie tunnelrief banner mural hworkerefinedOF countless sig em consideration church chose only Grants \" robotic relic see fru urging love palpable squeeze was unchfit 66 miganey sill styl eldest playuer Struggle runner convince Mister beer speedingouxSmall Sally words chasing suggestion dips lifeles op most Fo likes night HI Pacific bars lonely displays \"ChelseaChangedoling ride sym performances\" compartment 1932 delighted loudly timer are!] is Introduction knew inferred smilingaffle lith PASS sorry and DmitvedFree IMAGES shouldnut curious pushes391 shook humbleium barDT are Tipanda primer R meritsInterview chasing tocatch theDid Bars impending sit quick to himRemove concentrating could known heroes scratch eastern or generouschool real separatist are feared sphost is them stampsaughed Advertising but Vale founding coached snatched creative '' embarked stick caption\"( wanted pancwith conquer sourellyinclude Fukushima such introduces Let serving log Cargo of trainDec railway give.Strange\n",
      "\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m----> 2\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbos_token_id\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meos_idx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(token_ids_to_text(output, text_tokenizer\u001b[38;5;241m=\u001b[39mtokenizer))\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m50\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\charl\\anaconda3\\envs\\nanofm\\lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\charl\\OneDrive - Ecole Polytechnique\\EPFL\\Visual Intelligence\\Exercices\\CS-503-FM-Homework\\nanofm\\models\\gpt.py:207\u001b[0m, in \u001b[0;36mGPT.generate\u001b[1;34m(self, context, eos_idx, temp, top_p, top_k)\u001b[0m\n\u001b[0;32m    203\u001b[0m current_tokens \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([context], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)      \u001b[38;5;66;03m# shape (1, L)\u001b[39;00m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_seq_len \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(context)):\n\u001b[0;32m    205\u001b[0m     \n\u001b[0;32m    206\u001b[0m     \u001b[38;5;66;03m# Run a forward pass through the model to get the logits\u001b[39;00m\n\u001b[1;32m--> 207\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_tokens\u001b[49m\u001b[43m)\u001b[49m     \u001b[38;5;66;03m# Shape: (B, L, vocab_size) with batch size B=1 at inference time\u001b[39;00m\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;66;03m# Keep only the last token's logits and sample the next token\u001b[39;00m\n\u001b[0;32m    210\u001b[0m     \u001b[38;5;66;03m# Hint: Use the sample_tokens function from utils/sampling.py\u001b[39;00m\n\u001b[0;32m    211\u001b[0m     \u001b[38;5;66;03m# Make sure to pass the temperature, top_k and top_p arguments\u001b[39;00m\n\u001b[0;32m    212\u001b[0m     last_logit \u001b[38;5;241m=\u001b[39m logits[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]      \u001b[38;5;66;03m# Shape: (B, vocab_size) with batch size B=1 at inference time\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\charl\\OneDrive - Ecole Polytechnique\\EPFL\\Visual Intelligence\\Exercices\\CS-503-FM-Homework\\nanofm\\models\\gpt.py:135\u001b[0m, in \u001b[0;36mGPT.forward_model\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[38;5;66;03m# TODO: Pass to the output normalization and output projection layer to compute the logits\u001b[39;00m\n\u001b[0;32m    134\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_norm(x)\n\u001b[1;32m--> 135\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_logits\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;66;03m# TODO: Return the logits\u001b[39;00m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m logits\n",
      "File \u001b[1;32mc:\\Users\\charl\\anaconda3\\envs\\nanofm\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\charl\\anaconda3\\envs\\nanofm\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\charl\\anaconda3\\envs\\nanofm\\lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for _ in range(5):\n",
    "    output = model.generate(context=[tokenizer.bos_token_id], temp=1.0, top_p=0.0, top_k=0.0, eos_idx=tokenizer.eos_token_id)[0]\n",
    "    print(token_ids_to_text(output, text_tokenizer=tokenizer))\n",
    "    print('\\n' + '-'*50 + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d024a84c-7727-4c17-acb5-4b415531726c",
   "metadata": {},
   "source": [
    "We don't have to generate the stories in a completely unconditional way. Let's try to probe the model and see if it learned some world knowledge. \n",
    "If we probe the model with the phrase `Daisy was hungry, so she`, we should expect a continuation talking about her getting food. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf89eced-7cfd-42ef-a0d9-26758f7c543f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Daisy was hungry, so she asked her mum for something to eat. Her mum gave her napkins to hold together. Daisy looked at them and smiled, she had found one.\n",
      "\n",
      "Daisy and her mum sat together on the sofa. Daisy saw a cheese in a cookie. She was so excited and clapped her hands.\n",
      "\n",
      "Daisy's mum said, \"Let's make a cheese sandwich for you to eat.â€ She handed Daisy the napkins.\n",
      "\n",
      "Daisy's dad caught a hot cheese pie. He said, \"Here's a sweet and warm snack! vanish into thin pieces for you.\" Daisy was so happy.\n",
      "\n",
      "Daisy's mum said, \"That was very generous of us. I'm glad you shower your napkin!\" Daisy smiled and said, \"Me too!\"\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Daisy was hungry, so she went to the kitchen. She heard her mommy's voice and knew her mommy had something special for her. Daisy went to the kitchen and saw mommy cooking something warm and squ disgusting in the kitchen. Daisy was scared, but she went to help. Mommy said, \"What do you recognize?\" Daisy smiled and said, \"when I'm done cooking, mommy!\" Mommy smiled and thanked Daisy. She showed Daisy how to make spaghetti for dinner. Daisy was so proud of herself for helping her mommy, and she realized that grown throwing things out was not so bad.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Daisy was hungry, so she went to the kitchen. She saw a plate on the floor. It was a big, round plate with cheese on it. Daisy smiled and started to toes.\n",
      "\n",
      "She touched the plate. It felt cold and fuzzy, but she liked it. She moved on it, making it bounce. She wanted to get theSometimes sit, so she could eat the cheese.\n",
      "\n",
      "Then Daisy saw some crayons next to the big plate. She papered and drew pictures and shapes on the note. She had lots of fun admiring all the pictures.\n",
      "\n",
      "Soon Daisy was done. It was time for lunch. Daisy's mom had arrived in the kitchen with her steaming, delicious sandwiches.\n",
      "\n",
      "Daisy happily ate her lunch and threw away the crumbs. She was no longer hungry, but she was satisfied after all her adventure.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Daisy was hungry, so she got something yummy. It was a tomato. It was bright yellow, with yellow spots on it. Daisy loved tomatoes. She took a bite and it was so tasty!\n",
      "\n",
      "Suddenly, Daisy felt something happiest. Daisy looked up and saw the sun was shining down. The sun made the tomato even sweeter. Daisy smiled and thought the sun was happy too.\n",
      "\n",
      "Daisy said, \"Mommy, I'm so happy. The sun is shining starts toobs like us!\"\n",
      "\n",
      "Mommy smiled. She said, \"Yes, Daisy. The sun is giving us the time to be happy.\"\n",
      "\n",
      "With that, Daisy and Mommy went back to eating. Daisy was happy too, and danced around with her new heavy tomato.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Daisy was hungry, so she asked her mum for something to eat. Her mum made her a sandwich and her brother brought some milk and a w Rory. Daisy was so happy and ate the sandwich quickly.\n",
      "\n",
      "After she finished eating, Daisy asked her mum for another animal to throw. But her mum said no, it was time for lunch and Daisy felt embarrassed. Her face was yellow and she wanted a minute to cry.\n",
      "\n",
      "\"Come on Daisy,\" said her mum, \"the food will keep you both energy\". Daisy's heart was beating very fast and she felt a bit guilty, so she decided to listen to her mum and stopped complaining.\n",
      "\n",
      "Her mum said \"Porse today is interesting! There are lots of happy animals in the ocean\". Daisy smiled and felt much better.\n",
      "\n",
      "The next time Daisy made a request, her mum reminded her to stay full and not throw her food. Daisy said \"Ok!\", and smiled. She was very happy and she never felt guilty about not having a burger before.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "context = tokenizer.encode('Daisy was hungry, so she')[:-1] # Encode and discard automatically added [EOS] token\n",
    "\n",
    "for _ in range(5):\n",
    "    output = model.generate(context=context, temp=1.0, top_p=0.0, top_k=0.0, eos_idx=tokenizer.eos_token_id)[0]\n",
    "    print(token_ids_to_text(output, text_tokenizer=tokenizer))\n",
    "    print('\\n' + '-'*50 + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb687d48-e5ad-48a2-9e5b-e01099505d61",
   "metadata": {},
   "source": [
    "### 2.5 Open-ended questions (5 points each)\n",
    "\n",
    "Please answer the following questions. You may use additional cells to demonstrate your answers if necessary.\n",
    "\n",
    "- Q2.1: What effect does the temperature have on the generations?\n",
    "    - A2.1: [Please fill your answer here]\n",
    "- Q2.2: What about the top_k and top_p parameters?\n",
    "    - A2.2: [Please fill your answer here]\n",
    "- Q2.3: Sometimes the generations of this model are not very good. What could we do to improve it further?\n",
    "    - A2.3: [Please fill your answer here]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536ff7f9-f249-4412-a3f6-e60310db279a",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "## 3 Training nanoGPT on MNIST for conditional image generation\n",
    "\n",
    "In this part, we will train our nanoGPT on images, similar in spirit to [iGPT](https://openai.com/research/image-gpt).\n",
    "What this means is that we turn images into a sequence of discrete tokens and train a decoder-only Transformer to perform next-token-prediction.\n",
    "At inference time, we can then autoregressively generate new images from the training distribution, conditioned on the class label!\n",
    "\n",
    "If you have successfully completed the nanoGPT implementation, you can directly re-use it for this step! The only thing we change is the way we represent the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0d1209-9135-4254-929f-723b52862259",
   "metadata": {},
   "source": [
    "### 3.1 Tokenization\n",
    "\n",
    "First, we need to turn images into sequences of discrete tokens.\n",
    "\n",
    "A common way of performing tokenization of images is to train a discrete VAE, such as [VQ-GAN](https://arxiv.org/abs/2012.09841) to autoencode images through a discrete bottleneck. This is an entire research field of its own, so we will not go further into that here.\n",
    "\n",
    "Instead, we will tokenize the images in a much simpler way:\n",
    "- First, we turn the grayscale images into black and white → each image is now a sequence of 14x14 zeros and ones. In this manner, we would already have a sequence of discrete tokens we could model, but the sequence length of 14x14=192 could be shorter.\n",
    "- To reduce the sequence length further, we divide the image into 2x2 patches and turn each of the 2x2 patterns into a unique index. If we flatten each 2x2 patch into a sequence of 4 zeros and ones, we can directly interpret them as integers between 0 and 15. This reduces the sequence length by a factor of four. Much more manageable in our toy setting!\n",
    "\n",
    "While this way of tokenizing is really quite \"toy\", it is indicative of a common problem when dealing with images and transformers: naively turning images to tokens can result in very large sequence lengths.\n",
    "\n",
    "Let's plot some examples of the tokenized images. First is the original image, then we show each token, and finally we show the corresponding token sequence. \n",
    "The first token is the class-label, and the next 49 are the image tokens in raster-scan order. Their token id was shifted by 10 to account for the additional class token (we share the vocabulary with it)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c72a238c-283d-4d48-9f9b-dceb30e57729",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Can't pickle local object 'create_tokenized_mnist_dataloader.<locals>.collate_fn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m data_loader \u001b[38;5;241m=\u001b[39m create_tokenized_mnist_dataloader(train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, add_label_token\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m----> 2\u001b[0m data_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m      4\u001b[0m tokens \u001b[38;5;241m=\u001b[39m data_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      5\u001b[0m reconst \u001b[38;5;241m=\u001b[39m detokenize_MNIST(tokens, patch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, account_for_labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\charl\\anaconda3\\envs\\nanofm\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:491\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    489\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator\n\u001b[0;32m    490\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 491\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\charl\\anaconda3\\envs\\nanofm\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:422\u001b[0m, in \u001b[0;36mDataLoader._get_iterator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    420\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    421\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_worker_number_rationality()\n\u001b[1;32m--> 422\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_MultiProcessingDataLoaderIter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\charl\\anaconda3\\envs\\nanofm\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1146\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter.__init__\u001b[1;34m(self, loader)\u001b[0m\n\u001b[0;32m   1139\u001b[0m w\u001b[38;5;241m.\u001b[39mdaemon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1140\u001b[0m \u001b[38;5;66;03m# NB: Process.start() actually take some time as it needs to\u001b[39;00m\n\u001b[0;32m   1141\u001b[0m \u001b[38;5;66;03m#     start a process and pass the arguments over via a pipe.\u001b[39;00m\n\u001b[0;32m   1142\u001b[0m \u001b[38;5;66;03m#     Therefore, we only add a worker to self._workers list after\u001b[39;00m\n\u001b[0;32m   1143\u001b[0m \u001b[38;5;66;03m#     it started, so that we do not call .join() if program dies\u001b[39;00m\n\u001b[0;32m   1144\u001b[0m \u001b[38;5;66;03m#     before it starts, and __del__ tries to join but will get:\u001b[39;00m\n\u001b[0;32m   1145\u001b[0m \u001b[38;5;66;03m#     AssertionError: can only join a started process.\u001b[39;00m\n\u001b[1;32m-> 1146\u001b[0m \u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1147\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_queues\u001b[38;5;241m.\u001b[39mappend(index_queue)\n\u001b[0;32m   1148\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_workers\u001b[38;5;241m.\u001b[39mappend(w)\n",
      "File \u001b[1;32mc:\\Users\\charl\\anaconda3\\envs\\nanofm\\lib\\multiprocessing\\process.py:121\u001b[0m, in \u001b[0;36mBaseProcess.start\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _current_process\u001b[38;5;241m.\u001b[39m_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemon\u001b[39m\u001b[38;5;124m'\u001b[39m), \\\n\u001b[0;32m    119\u001b[0m        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemonic processes are not allowed to have children\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    120\u001b[0m _cleanup()\n\u001b[1;32m--> 121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sentinel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen\u001b[38;5;241m.\u001b[39msentinel\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;66;03m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\charl\\anaconda3\\envs\\nanofm\\lib\\multiprocessing\\context.py:224\u001b[0m, in \u001b[0;36mProcess._Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[1;32m--> 224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mProcess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\charl\\anaconda3\\envs\\nanofm\\lib\\multiprocessing\\context.py:336\u001b[0m, in \u001b[0;36mSpawnProcess._Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    333\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[0;32m    335\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpopen_spawn_win32\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Popen\n\u001b[1;32m--> 336\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\charl\\anaconda3\\envs\\nanofm\\lib\\multiprocessing\\popen_spawn_win32.py:93\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[1;34m(self, process_obj)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     92\u001b[0m     reduction\u001b[38;5;241m.\u001b[39mdump(prep_data, to_child)\n\u001b[1;32m---> 93\u001b[0m     \u001b[43mreduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mto_child\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     95\u001b[0m     set_spawning_popen(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\charl\\anaconda3\\envs\\nanofm\\lib\\multiprocessing\\reduction.py:60\u001b[0m, in \u001b[0;36mdump\u001b[1;34m(obj, file, protocol)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdump\u001b[39m(obj, file, protocol\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     59\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''Replacement for pickle.dump() using ForkingPickler.'''\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m     \u001b[43mForkingPickler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mAttributeError\u001b[0m: Can't pickle local object 'create_tokenized_mnist_dataloader.<locals>.collate_fn'"
     ]
    }
   ],
   "source": [
    "data_loader = create_tokenized_mnist_dataloader(train=False, add_label_token=True, shuffle=False)\n",
    "data_dict = next(iter(data_loader))\n",
    "\n",
    "tokens = data_dict['input_ids']\n",
    "reconst = detokenize_MNIST(tokens, patch_size=2, account_for_labels=True)\n",
    "bits = rearrange(reconst, 'b (nh ph) (nw pw) -> b (nh nw) ph pw', ph=2, pw=2)\n",
    "\n",
    "for i in range(2):\n",
    "    plt.figure(figsize=(5., 5.)); plt.imshow(reconst[i], cmap='Greys'); plt.show()\n",
    "    \n",
    "    fig = plt.figure(figsize=(5., 5.))\n",
    "    grid = ImageGrid(fig, 111, nrows_ncols=(7, 7), axes_pad=0.1)\n",
    "    for j, img in enumerate(bits[i]):\n",
    "        grid[j].imshow(img, cmap='Greys', vmin=0, vmax=1)\n",
    "    plt.show()\n",
    "    \n",
    "    print('Tokens:', tokens[i], '\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a78d4a-6df4-4d20-8627-a393ddf9634d",
   "metadata": {},
   "source": [
    "### 3.2 Training the model\n",
    "\n",
    "We defined a training config for you in: `cfgs/nanoGPT/mnist_d8w512.yaml`. Please familiarize yourself with all parts.\n",
    "Please don't forget to replace the Weights & Bias entity with your own.\n",
    "\n",
    "On a 1xV100 node, you can start the training like:\n",
    "```\n",
    "OMP_NUM_THREADS=1 torchrun --nproc_per_node=1 run_training.py --config cfgs/nanoGPT/mnist_d8w512.yaml\n",
    "```\n",
    "\n",
    "This training should be pretty fast and only take a few minutes. You should reach a final validation loss below 0.45, and your loss curves should look something like the following:\n",
    "\n",
    "<img src=\"./assets/nanoGPT_mnist.png\" alt=\"nanoGPT MNIST loss curves\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92233937-b07b-4313-8004-b17e058ca380",
   "metadata": {},
   "source": [
    "### 3.3 Show your loss curves (10 points)\n",
    "\n",
    "Screenshot your loss curves and show them here. Add the image to the `assets` directory and change the path in the markdown. You will get 10 points for reasonable loss curves (similar to the sample loss curves above).\n",
    "\n",
    "<img src=\"./assets/loss_curves_nanoGPT_mnist.png\" alt=\"nanoGPT MNIST loss curves\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5afdd8d6-a5fe-46a3-ba4e-7ee872d495ff",
   "metadata": {},
   "source": [
    "### 3.4 Evaluating the model (10 points)\n",
    "\n",
    "After you completed the training, load the model with the following cell. You may need to adjust the path if you changed it.\n",
    "You will get 10 points if the outputs look reasonable (similar to the sample outputs provided below).\n",
    "\n",
    "Hint: You can also load intermediate safetensors checkpoints to check the progress during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cede1ce4-dfff-4a99-aa8d-1761f2141c5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25.174528M parameters\n"
     ]
    }
   ],
   "source": [
    "ckpt_path = './outputs/nanoGPT/mnist_d8w512/checkpoint-final.safetensors'\n",
    "model = load_model_from_safetensors(ckpt_path, device=device)\n",
    "print(f'{model.get_num_params() / 10**6}M parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca54f04f-d106-4cef-b23b-936a91854bd9",
   "metadata": {},
   "source": [
    "Let's plot some class-conditional generations! We seed the generation by providing the first token, whose index is equal to the number we'd like to generate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "989eecc3-7c72-4a7f-a541-862ab263bc06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 5, 10, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 13, 12, 10, 10, 10, 15,\n",
       "         10, 10, 10, 10, 10, 10, 15, 25, 12, 10, 10, 10, 10, 10, 10, 23, 10, 10,\n",
       "         10, 10, 13, 11, 17, 10, 10, 10, 10, 14, 22, 18, 10, 10]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label = 5\n",
    "output = model.generate(context=[label], temp=0.7, top_p=0.0)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "93a41606-6524-47e4-90ea-ab0b271f0504",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1e09dfa9bd0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaAAAAGdCAYAAABU0qcqAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjEsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvc2/+5QAAAAlwSFlzAAAPYQAAD2EBqD+naQAAFvhJREFUeJzt3XuMFfX5+PFngbJQwq6ClUsEpYYEFaRWxXhJWyPRGILSRq0GK8WkTQxW0cQibcEaL6u2NURL8JLU2kS8/CFoTbSxeCFGEBA1Nm0RIkGiQTTRXcCwEjjfzOQHP1ex3s7y7Jx9vZLJei7LzLi78z4z8zlzmmq1Wi0A4ADrc6BnCAAFAQIghQABkEKAAEghQACkECAAUggQACkECIAU/aKH2bNnT7zzzjsxePDgaGpqyl4cAL6i4voG27Zti5EjR0afPn2qE6AiPqNGjcpeDAC+oc2bN8dhhx1WnQAVez57F7ylpSV7cQD4ijo6Osodib3b88oEaO9htyI+AgRQXV90GsUgBABSCBAAKQQIgBQCBEAKAQIghQAB0FgBWrhwYRxxxBExYMCAOOmkk2LVqlXdNSsAKqhbAvTwww/H1VdfHdddd12sXbs2Jk6cGGeddVZs3bq1O2YHQAV1S4Buv/32+MUvfhEzZ86Mo48+Ou6666749re/HX/5y1+6Y3YAVFDdA/Txxx/Hyy+/HJMnT/7/M+nTp7y9YsWKzzy/s7OzvGzDJycAGl/dA/T+++/H7t27Y9iwYV3uL25v2bLlM89va2uL1tbWfZMLkQL0Dumj4ObOnRvt7e37puIipAA0vrpfjPSQQw6Jvn37xrvvvtvl/uL28OHDP/P85ubmcgKgd6n7HlD//v3j+OOPj2XLlnX5kLni9sknn1zv2QFQUd3ycQzFEOwZM2bECSecEJMmTYoFCxbEjh07ylFxANBtAfrpT38a7733XsyfP78cePC9730vnnrqqc8MTACg92qqFR/e3YMUw7CL0XDFgAQfSAdQPV92O54+Cg6A3kmAAEghQACkECAAUggQACkECIAUAgRACgECIIUAAZBCgABIIUAApBAgAFIIEAApBAiAFAIEQAoBAiCFAAGQQoAASCFAAKQQIABSCBAAKQQIgBQCBEAKAQIghQABkEKAAEghQACkECAAUggQACkECIAUAgRACgECIIUAAZBCgABIIUAApBAgAFIIEAApBAiAFAIEQAoBAiCFAAGQQoAAaIwAtbW1xYknnhiDBw+OQw89NKZNmxbr1q2r92wAqLi6B+j555+PWbNmxcqVK+Ppp5+OXbt2xZlnnhk7duyo96wAqLCmWq1W684ZvPfee+WeUBGmH/zgB1/4/I6OjmhtbY329vZoaWnpzkUDoBt82e14v+hmxQIUhgwZst/HOzs7y+mTCw5A4+vWQQh79uyJ2bNnx6mnnhrjx4//3HNGRSn3TqNGjerORQKgNxyCu+yyy+LJJ5+MF154IQ477LAvvQdURMghOIBqSj8Ed/nll8cTTzwRy5cv/9z4FJqbm8sJgN6l7gEqdqh+9atfxZIlS+K5556LMWPG1HsWADSAugeoGIK9ePHieOyxx8r3Am3ZsqW8v9gdGzhwYL1nB0BF1f0cUFNT037vv+++++LnP//5F36/YdgA1ZZ2Dqib31YEQINwLTgAUggQACkECIAUAgRACgECIEW3X4wUumtof5VHcB6odeKbMaq3e9kDAiCFAAGQQoAASCFAAKQQIABSCBAAKQQIgBQCBEAKAQIghQABkEKAAEghQACkECAAUggQACkECIAUAgRACgECIIUAAZBCgABIIUAApBAgAFIIEAApBAiAFAIEQAoBAiCFAAGQol/ObKF3q9Vq2YsA6ewBAZBCgABIIUAApBAgAFIIEAApBAiAFAIEQAoBAqAxA3TLLbdEU1NTzJ49u7tnBUCFdGuAVq9eHXfffXcce+yx3TkbACqo2wK0ffv2mD59etx7771x8MEHd9dsAKiobgvQrFmzYsqUKTF58uT/+bzOzs7o6OjoMgHQ+LrlYqQPPfRQrF27tjwE90Xa2tri+uuv747FAKA37QFt3rw5rrzyynjggQdiwIABX/j8uXPnRnt7+76p+H4AGl9Trc7XhV+6dGn8+Mc/jr59++67b/fu3eVIuD59+pSH3D752KcVh+BaW1vLGLW0tNRz0aio4nfnQPARCVAfX3Y7XvdDcGeccUa8/vrrXe6bOXNmjBs3LubMmfM/4wNA71H3AA0ePDjGjx/f5b5BgwbF0KFDP3M/AL2XKyEA0Lgfyf3cc88diNkAUCH2gABIIUAApBAgAFIIEAApBAiAFAIEQAoBAiCFAAGQQoAASCFAAKQQIABSCBAAKQQIgBQCBEAKAQIghQABkEKAAEghQACkECAAUggQACkECIAUAgRACgECIIUAAZBCgABIIUAApBAgAFIIEAApBAiAFAIEQAoBAiCFAAGQQoAASNEvZ7bQ8zQ1NUWjqdVq2YsAn8seEAApBAiAFAIEQAoBAiCFAAGQQoAASCFAAKQQIABSdEuA3n777bj44otj6NChMXDgwJgwYUKsWbOmO2YFQEXV/UoIH3zwQZx66qlx+umnx5NPPhnf+c53Yv369XHwwQfXe1YAVFjdA3TrrbfGqFGj4r777tt335gxY+o9GwAqru6H4B5//PE44YQT4vzzz49DDz00jjvuuLj33ns/9/mdnZ3R0dHRZQKg8dU9QG+++WYsWrQoxo4dG//4xz/isssuiyuuuCLuv//+/T6/ra0tWltb903F3hMAja+pVufL5fbv37/cA3rxxRf33VcEaPXq1bFixYr97gEV017FHlARofb29mhpaannolFRjXiV6gPF1bDJUGzHix2KL9qO130PaMSIEXH00Ud3ue+oo46Kt956a7/Pb25uLhfwkxMAja/uASpGwK1bt67LfW+88UYcfvjh9Z4VABVW9wBdddVVsXLlyrj55ptjw4YNsXjx4rjnnnti1qxZ9Z4VABVW9wCdeOKJsWTJknjwwQdj/PjxccMNN8SCBQti+vTp9Z4VABVW90EIB+rkFb2HQQhfXw/786aX6MgahAAAX4YAAZBCgABIIUAApBAgAFIIEACN8XEMQO8cwm7IN1+VPSAAUggQACkECIAUAgRACgECIIUAAZBCgABIIUAApBAgAFIIEAApBAiAFAIEQAoBAiCFAAGQQoAASCFAAKQQIABSCBAAKQQIgBQCBEAKAQIghQABkEKAAEghQACkECAAUvTLmS18ebVaLXsRKqupqanh5uX3oXHYAwIghQABkEKAAEghQACkECAAUggQACkECIAUAgRACgECoDECtHv37pg3b16MGTMmBg4cGEceeWTccMMN3r0MQPdeiufWW2+NRYsWxf333x/HHHNMrFmzJmbOnBmtra1xxRVX1Ht2AFRU3QP04osvxrnnnhtTpkwpbx9xxBHx4IMPxqpVq+o9KwAqrO6H4E455ZRYtmxZvPHGG+Xt1157LV544YU4++yz9/v8zs7O6Ojo6DIB0Pjqvgd07bXXlhEZN25c9O3btzwndNNNN8X06dP3+/y2tra4/vrr670YAPS2PaBHHnkkHnjggVi8eHGsXbu2PBf0xz/+sfy6P3Pnzo329vZ90+bNm+u9SAD0hj2ga665ptwLuvDCC8vbEyZMiE2bNpV7OjNmzPjM85ubm8sJgN6l7ntAH330UfTp0/WfLQ7F7dmzp96zAqDC6r4HNHXq1PKcz+jRo8th2K+88krcfvvtcemll9Z7VgBUWFOtzu8Q3bZtW/lG1CVLlsTWrVtj5MiRcdFFF8X8+fOjf//+X/j9xQCG4j1DxfmglpaWei4a9DoH8iO5DxRvau/5vux2vO4B+qYECOpHgOjJ23HXggMghQABkEKAAEghQACkECAAUggQAI3xRlSgdw5ZbsQh33Qve0AApBAgAFIIEAApBAiAFAIEQAoBAiCFAAGQQoAASCFAAKQQIABSCBAAKQQIgBQCBEAKAQIghQABkEKAAEghQACkECAAUggQACkECIAUAgRACgECIIUAAZBCgABIIUAApBAgAFIIEAApBAiAFAIEQAoBAiCFAAGQQoAASCFAAKQQIABSCBAA1QjQ8uXLY+rUqTFy5MhoamqKpUuXdnm8VqvF/PnzY8SIETFw4MCYPHlyrF+/vp7LDEBvDNCOHTti4sSJsXDhwv0+ftttt8Udd9wRd911V7z00ksxaNCgOOuss2Lnzp31WF4AGkS/r/oNZ599djntT7H3s2DBgvjd734X5557bnnf3/72txg2bFi5p3ThhRd+8yUGoCHU9RzQxo0bY8uWLeVht71aW1vjpJNOihUrVuz3ezo7O6Ojo6PLBEDjq2uAivgUij2eTypu733s09ra2spI7Z1GjRpVz0UCoIdKHwU3d+7caG9v3zdt3rw5e5EAqFqAhg8fXn599913u9xf3N772Kc1NzdHS0tLlwmAxlfXAI0ZM6YMzbJly/bdV5zTKUbDnXzyyfWcFQC9bRTc9u3bY8OGDV0GHrz66qsxZMiQGD16dMyePTtuvPHGGDt2bBmkefPmle8ZmjZtWr2XHYDeFKA1a9bE6aefvu/21VdfXX6dMWNG/PWvf41f//rX5XuFfvnLX8aHH34Yp512Wjz11FMxYMCA+i45AJXWVCvevNODFIfsitFwxYAE54OgOooroxwIPWyTxTfYjqePggOgdxIgAFIIEAApBAiAFAIEQAoBAqAa7wOCAz3s9kA5kMN7G+3/HXwd9oAASCFAAKQQIABSCBAAKQQIgBQCBEAKAQIghQABkEKAAEghQACkECAAUggQACkECIAUAgRACgECIIUAAZBCgABIIUAApBAgAFIIEAApBAiAFAIEQAoBAiCFAAGQQoAASNEvZ7Y0glqtdkDm09TU1FDzaVQH6veBxmEPCIAUAgRACgECIIUAAZBCgABIIUAApBAgAFIIEAApBAiAagRo+fLlMXXq1Bg5cmT5zvGlS5fue2zXrl0xZ86cmDBhQgwaNKh8ziWXXBLvvPNOvZcbgN4WoB07dsTEiRNj4cKFn3nso48+irVr18a8efPKr48++misW7cuzjnnnHotLwANoqn2DS7gVOwBLVmyJKZNm/a5z1m9enVMmjQpNm3aFKNHj/7Cf7OjoyNaW1ujvb09Wlpavu6i0UBco60aXAuOr7od7/aLkRYLUGxADjrooP0+3tnZWU6fXHAAGl+3DkLYuXNneU7ooosu+twKtrW1laXcO40aNao7FwmARg9QMSDhggsuKHfLFy1a9LnPmzt3brmXtHfavHlzdy0SAD1Iv+6MT3He55lnnvmfxwCbm5vLCYDepV93xWf9+vXx7LPPxtChQ+s9CwB6Y4C2b98eGzZs2Hd748aN8eqrr8aQIUNixIgRcd5555VDsJ944onYvXt3bNmypXxe8Xj//v3ru/QA9J5h2M8991ycfvrpn7l/xowZ8fvf/z7GjBmz3+8r9oZ+9KMffeG/bxg2n2YYdjUYhk23D8MuIvK/ftH8EgLwZbgWHAApBAiAFAIEQAoBAiCFAAGQQoAASNHtV8OGb8rQfmhM9oAASCFAAKQQIABSCBAAKQQIgBQCBEAKAQIghQABkEKAAEghQACkECAAUggQACkECIAUAgRACgECIIUAAZBCgABIIUAApBAgAFIIEAApBAiAFAIEQAoBAiCFAAGQol/0MLVarfza0dGRvSgAfA17t997t+eVCdC2bdvKr6NGjcpeFAC+4fa8tbX1cx9vqn1Rog6wPXv2xDvvvBODBw+Opqamr1TcIlqbN2+OlpaWqLpGW5+CdaoG69TzdfTw9SmyUsRn5MiR0adPn+rsARULe9hhh33t7y9+GD3xB/J1Ndr6FKxTNVinnq+lB6/P/9rz2csgBABSCBAAKRomQM3NzXHdddeVXxtBo61PwTpVg3Xq+ZobZH163CAEAHqHhtkDAqBaBAiAFAIEQAoBAiBFQwRo4cKFccQRR8SAAQPipJNOilWrVkVVtbW1xYknnlheCeLQQw+NadOmxbp166JR3HLLLeUVLmbPnh1V9/bbb8fFF18cQ4cOjYEDB8aECRNizZo1UUW7d++OefPmxZgxY8p1OfLII+OGG274wmt59STLly+PqVOnlu++L37Hli5d2uXxYl3mz58fI0aMKNdx8uTJsX79+qjqOu3atSvmzJlT/t4NGjSofM4ll1xSXkmmKiofoIcffjiuvvrqckji2rVrY+LEiXHWWWfF1q1bo4qef/75mDVrVqxcuTKefvrp8pfszDPPjB07dkTVrV69Ou6+++449thjo+o++OCDOPXUU+Nb3/pWPPnkk/Hvf/87/vSnP8XBBx8cVXTrrbfGokWL4s9//nP85z//KW/fdtttceedd0ZVFH8jxd9/8YJ0f4r1ueOOO+Kuu+6Kl156qdxoF9uKnTt3RhXX6aOPPiq3ecULh+Lro48+Wr5YPeecc6IyahU3adKk2qxZs/bd3r17d23kyJG1tra2WiPYunVr8RK09vzzz9eqbNu2bbWxY8fWnn766doPf/jD2pVXXlmrsjlz5tROO+20WqOYMmVK7dJLL+1y309+8pPa9OnTa1VU/M0sWbJk3+09e/bUhg8fXvvDH/6w774PP/yw1tzcXHvwwQdrVVyn/Vm1alX5vE2bNtWqoNJ7QB9//HG8/PLL5a70J68lV9xesWJFNIL29vby65AhQ6LKir26KVOmdPlZVdnjjz8eJ5xwQpx//vnlodLjjjsu7r333qiqU045JZYtWxZvvPFGefu1116LF154Ic4+++xoBBs3bowtW7Z0+f0rrlVWHLJvlG3F3u1FcajuoIMOiirocRcj/Sref//98tj1sGHDutxf3P7vf/8bVVdcGbw4V1Ic6hk/fnxU1UMPPVQeIigOwTWKN998szxkVRz+/c1vflOu2xVXXBH9+/ePGTNmRNVce+215RWWx40bF3379i3/rm666aaYPn16NIIiPoX9bSv2PlZ1O3fuLM8JXXTRRT32AqUNFaBGV+w1/Otf/ypfiVZVcbn4K6+8sjyfVQwSaRTFi4NiD+jmm28ubxd7QMXPqji/UMUAPfLII/HAAw/E4sWL45hjjolXX321fPFTnNiu4vr0Nrt27YoLLrigHGhRvDCqikofgjvkkEPKV2vvvvtul/uL28OHD48qu/zyy+OJJ56IZ5999ht9PEW24hBpMSDk+9//fvTr16+cioEWxcng4r+LV9pVVIykOvroo7vcd9RRR8Vbb70VVXTNNdeUe0EXXnhhOarqZz/7WVx11VXlqMxGsHd70Ijbil3/Lz6bNm0qX+hVZe+n8gEqDnccf/zx5bHrT74yLW6ffPLJUUXFK5giPkuWLIlnnnmmHBZbZWeccUa8/vrr5SvqvVOx51Ac2in+u3gBUUXFYdFPD48vzp8cfvjhUUXFiKpPf3BY8bMp/p4aQfF3VITmk9uK4pBjMRquqtuKT8anGE7+z3/+s3xLQJVU/hBccQy+OERQbNQmTZoUCxYsKIcuzpw5M6p62K04DPLYY4+V7wXae3y6OGFavHehaop1+PT5q2L4a/GHUuXzWsXeQXHivjgEV2wAivee3XPPPeVURcV7TYpzPqNHjy4Pwb3yyitx++23x6WXXhpVsX379tiwYUOXgQfFi5xiAE+xXsUhxRtvvDHGjh1bBqkYvlwcYizea1fFdRoxYkScd9555fnV4mhJcTRh7/aieLx4gd7j1RrAnXfeWRs9enStf//+5bDslStX1qqq+JHsb7rvvvtqjaIRhmEX/v73v9fGjx9fDuUdN25c7Z577qlVVUdHR/kzKf6OBgwYUPvud79b++1vf1vr7OysVcWzzz6737+dGTNm7BuKPW/evNqwYcPKn9kZZ5xRW7duXa2q67Rx48bP3V4U31cFPo4BgBSVPgcEQHUJEAApBAiAFAIEQAoBAiCFAAGQQoAASCFAAKQQIABSCBAAKQQIgBQCBEBk+D/3NsJsmPoUowAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reconst = detokenize_MNIST(output, patch_size=2, account_for_labels=True).cpu()\n",
    "plt.imshow(reconst[0], cmap='gray_r')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e0c051-cf4f-42ba-ba02-716ad3d2add2",
   "metadata": {},
   "source": [
    "Let's now generate 10 random samples for all 10 classes. Most should look quite reasonable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0ca0356c-eb7e-4cab-b242-27525d6a3b38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 200 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def generate_samples(model, temp=1.0, top_p=0.0, top_k=0.0, n_samples=10):\n",
    "    fig = plt.figure(figsize=(8., 8.))\n",
    "    grid = ImageGrid(fig, 111, nrows_ncols=(10, n_samples), axes_pad=0.1)\n",
    "    for label in range(10):\n",
    "        for sample_idx in range(n_samples):\n",
    "            grid_idx = label * n_samples + sample_idx\n",
    "            output = model.generate(context=[label], temp=temp, top_p=top_p, top_k=top_k)\n",
    "            reconst = detokenize_MNIST(output, patch_size=2, account_for_labels=True).cpu()\n",
    "            grid[grid_idx].imshow(reconst[0], cmap='Greys', vmin=0, vmax=1)\n",
    "    plt.show()\n",
    "    \n",
    "generate_samples(model, temp=0.7, top_p=0.9, top_k=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29571b8-ebaf-4ef1-8426-4863e986fff6",
   "metadata": {},
   "source": [
    "### 3.5 Open-ended questions (5 points each)\n",
    "\n",
    "Please answer the following questions. You may use additional cells to demonstrate your answers if necessary.\n",
    "\n",
    "- Q3.1: What effect does the temperature have on the generations?\n",
    "    - A3.1: [Please fill your answer here]\n",
    "- Q3.2: What about the top_k and top_p parameters?\n",
    "    - A3.2: [Please fill your answer here]\n",
    "- Q3.3: How might we extend this to text-to-image?\n",
    "    - A3.3: [Please fill your answer here]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8908f8-c491-4519-88c3-72da701e6933",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 4 Further reading\n",
    "\n",
    "Having implemented causal-attention, as well as a decoder-only Transformer, you should not have a hard time implementing cross-attention and a full encoder-decoder Transformer to train on arbitrary sequence-to-sequence tasks.\n",
    "We will explore these topics in the next weeks.\n",
    "That said, here is some further reading material should you want to dive deeper."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbff92e-3808-41b0-a4fe-c3746b6acfdc",
   "metadata": {},
   "source": [
    "### 4.1 Papers & Blogs\n",
    "\n",
    "- [Attention Is All You Need, Vaswani et al. 2017 (Original Transformer paper)](https://arxiv.org/abs/1706.03762)\n",
    "- [Language Models are Unsupervised Multitask Learners, Radford et al. 2018 (GPT-2 paper)](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)\n",
    "- [Language Models are Few-Shot Learners, Brown et al. 2020 (GPT-3 paper)](https://arxiv.org/abs/2005.14165)\n",
    "- [PaLM: Scaling Language Modeling with Pathways, Chrowdherry et al. 2022](https://arxiv.org/abs/2204.02311)\n",
    "- [LLaMA: Open and Efficient Foundation Language Models, Touvron et al. 2023](https://arxiv.org/abs/2302.13971)\n",
    "- [Llama 2: Open Foundation and Fine-Tuned Chat Models, Touvron et al. 2023](https://arxiv.org/abs/2307.09288)\n",
    "- [The Llama 3 Herd of Models](https://arxiv.org/abs/2407.21783)\n",
    "- [Scaling Laws for Neural Language Models, Kaplan et al. 2020](https://arxiv.org/abs/2001.08361)\n",
    "- [Training Compute-Optimal Large Language Models, Hoffmann et al. (Chinchilla)](https://arxiv.org/abs/2203.15556)\n",
    "- [Pixel Recurrent Neural Networks, Van den Oord et al. 2016 (PixelCNN)](https://arxiv.org/abs/1601.06759)\n",
    "- [PixelCNN++: Improving the PixelCNN with Discretized Logistic Mixture Likelihood and Other Modifications, Salimans et al. 2017](https://arxiv.org/abs/1701.05517)\n",
    "- [Zero-Shot Text-to-Image Generation, Ramesh et al. 2021 (DALL-E 1)](https://arxiv.org/abs/2102.12092)\n",
    "- [Autoregressive Model Beats Diffusion: Llama for Scalable Image Generation, Sun et al. 2024 (Image generation with simple autoregressive models)](https://arxiv.org/abs/2406.06525)\n",
    "- [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale, Dosovitskiy et al. 2020 (Vision Transformer paper)](https://arxiv.org/abs/2010.11929)\n",
    "- [Transformer Circuits Thread](https://transformer-circuits.pub/)\n",
    "- [The Transformer Family Version 2.0, Lilian Weng 2023](https://lilianweng.github.io/posts/2023-01-27-the-transformer-family-v2/)\n",
    "- [The Illustrated Transformer, Jay Alammar 2018](https://jalammar.github.io/illustrated-transformer/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9ec4f0-9057-4a8c-ac92-dc9603c5012c",
   "metadata": {},
   "source": [
    "### 4.2 Code bases\n",
    "\n",
    "- [nanoGPT](https://github.com/karpathy/nanoGPT)\n",
    "- [Llama model inference code](https://github.com/meta-llama/llama-models)\n",
    "- [LlamaGen](https://github.com/FoundationVision/LlamaGen)\n",
    "- [torchtitan](https://github.com/pytorch/torchtitan)\n",
    "- [lingua](https://github.com/facebookresearch/lingua)\n",
    "\n",
    "You might find that, in many ways, these repos are not that much different from what we implemented here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1e5682-f0d7-415c-9c7c-6aae68b0fe6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nanofm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
