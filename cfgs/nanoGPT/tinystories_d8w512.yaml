# Designed to be run on 2xV100-32GB GPUs

run_name: auto # Auto-generate a run name
output_dir: ./outputs/auto # Set to auto to use the run_name

# Global variables used throughout the config
global_vars:
  batch_size: 128 # per GPU
  max_seq_len: 256

# Training
batch_size: ${global_vars.batch_size}
total_tokens: 1000 # in millions of tokens
warmup_tokens: 100 # in millions of tokens
num_tokens_per_sample: ${global_vars.max_seq_len} # Number of tokens per sample to count towards the total tokens seen
lr: 0.0003 # Max learning rate
min_lr: 0.000001 # Min learning rate after cosine decay
weight_decay: 0.05 # AdamW weight decay
clip_grad: 1.0 # Gradient clipping norm
dtype: fp16 # Precision. Choose fp16 on V100s, bf16 on A100s or later. In case of instability, try fp32.

# Eval
eval_freq: 100 # in millions of tokens
save_ckpt_freq: 200 # in millions of tokens

# Logging
log_wandb: True
wandb_project: COM304_nanoGPT # wandb project name
wandb_entity: <YOUR_WANDB_ENTITY_NAME> # Set to your wandb username
wandb_run_name: auto # Set to auto to use the run_name

# Model config
model_config:
  _target_: nanofm.models.gpt.GPT
  seq_read_key: input_ids # Key to read the input sequence from the batch
  dim: 512 # Model dimension
  depth: 8 # Number of Transformer layers
  head_dim: 64 # Dim of each attention head
  vocab_size: 50304 # gpt2 tokenizer has 50257 tokens, plus special tokens, padded to nearest multiple of 64 for efficiency
  max_seq_len: ${global_vars.max_seq_len}
  padding_idx: 50257 # Defined in the tokenizer

# Train loader config
train_loader_config:
  _target_: nanofm.data.text.huggingface_datasets.create_hf_dataloader
  dataset_id: roneneldan/TinyStories
  split: train
  tokenizer_path: gpt2
  add_padding_tokens: True # Add [PAD] tokens
  add_sos_eos_tokens: True # Add [SOS] and [EOS] tokens
  batch_size: ${global_vars.batch_size}
  max_seq_len: ${global_vars.max_seq_len}
  increase_seq_len_by_one: True # Loader returns 256+1 tokens to shift inputs/targets by one
  infinite: True
  distributed: True
  num_workers: 10
  pin_memory: True
  shuffle: True

# Eval loader config
eval_loader_config:
  _target_: nanofm.data.text.huggingface_datasets.create_hf_dataloader
  dataset_id: roneneldan/TinyStories
  split: validation
  tokenizer_path: gpt2
  add_padding_tokens: True # Add [PAD] tokens
  add_sos_eos_tokens: True # Add [SOS] and [EOS] tokens
  batch_size: ${global_vars.batch_size}
  max_seq_len: ${global_vars.max_seq_len}
  increase_seq_len_by_one: True # Loader returns 256+1 tokens to shift inputs/targets by one
  infinite: False
  distributed: True
  num_workers: 10
  pin_memory: True
  shuffle: False